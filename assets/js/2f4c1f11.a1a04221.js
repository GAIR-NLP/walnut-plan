"use strict";(self.webpackChunkwalnut_plan=self.webpackChunkwalnut_plan||[]).push([[8671],{2968:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Introduction","href":"/walnut-plan/docs/intro","docId":"intro","unlisted":false},{"type":"category","label":"Projects","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"O1 Replication Journey","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Overview","href":"/walnut-plan/docs/projects/o1_replicate/overview","docId":"projects/o1_replicate/overview","unlisted":false},{"type":"link","label":"Journey Learning vs Shortcut Learning","href":"/walnut-plan/docs/projects/o1_replicate/journey_learning","docId":"projects/o1_replicate/journey_learning","unlisted":false},{"type":"category","label":"Exploration Journey","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Outline","href":"/walnut-plan/docs/projects/o1_replicate/exploration_journey/outline","docId":"projects/o1_replicate/exploration_journey/outline","unlisted":false},{"type":"link","label":"What does O1\u2019s Thought Look Like?","href":"/walnut-plan/docs/projects/o1_replicate/exploration_journey/o1_thought","docId":"projects/o1_replicate/exploration_journey/o1_thought","unlisted":false},{"type":"link","label":"How does Long Thought Work?","href":"/walnut-plan/docs/projects/o1_replicate/exploration_journey/longthought_work","docId":"projects/o1_replicate/exploration_journey/longthought_work","unlisted":false},{"type":"link","label":"How to Construct Long Thoughts?","href":"/walnut-plan/docs/projects/o1_replicate/exploration_journey/construct_longthought","docId":"projects/o1_replicate/exploration_journey/construct_longthought","unlisted":false},{"type":"link","label":"How to Construct Reward Model?","href":"/walnut-plan/docs/projects/o1_replicate/exploration_journey/reward_model","docId":"projects/o1_replicate/exploration_journey/reward_model","unlisted":false},{"type":"link","label":"How to Construct an On-policy Reasoning Tree?","href":"/walnut-plan/docs/projects/o1_replicate/exploration_journey/construct_policy_tree","docId":"projects/o1_replicate/exploration_journey/construct_policy_tree","unlisted":false},{"type":"link","label":"How to Derive a Long Thought from a Reasoning Tree?","href":"/walnut-plan/docs/projects/o1_replicate/exploration_journey/tree2thought","docId":"projects/o1_replicate/exploration_journey/tree2thought","unlisted":false},{"type":"link","label":"How to Evaluate our Trials?","href":"/walnut-plan/docs/projects/o1_replicate/exploration_journey/eval_trial","docId":"projects/o1_replicate/exploration_journey/eval_trial","unlisted":false},{"type":"link","label":"How to Train our Models?","href":"/walnut-plan/docs/projects/o1_replicate/exploration_journey/train","docId":"projects/o1_replicate/exploration_journey/train","unlisted":false},{"type":"link","label":"What would be an effective annotation strategy for human-ai collaboration?","href":"/walnut-plan/docs/projects/o1_replicate/exploration_journey/human_annotation","docId":"projects/o1_replicate/exploration_journey/human_annotation","unlisted":false}],"href":"/walnut-plan/docs/category/exploration-journey"},{"type":"link","label":"Examples","href":"/walnut-plan/docs/projects/o1_replicate/examples","docId":"projects/o1_replicate/examples","unlisted":false},{"type":"link","label":"Future Plan","href":"/walnut-plan/docs/projects/o1_replicate/future_plan","docId":"projects/o1_replicate/future_plan","unlisted":false}],"href":"/walnut-plan/docs/category/o1-replication-journey"},{"type":"link","label":"Stay tuned ...","href":"/walnut-plan/docs/projects/stay_tune","docId":"projects/stay_tune","unlisted":false}],"href":"/walnut-plan/docs/category/projects"}]},"docs":{"intro":{"id":"intro","title":"Introduction","description":"Background","sidebar":"tutorialSidebar"},"projects/o1_replicate/examples":{"id":"projects/o1_replicate/examples","title":"Examples","description":"Case1","sidebar":"tutorialSidebar"},"projects/o1_replicate/exploration_journey/construct_longthought":{"id":"projects/o1_replicate/exploration_journey/construct_longthought","title":"How to Construct Long Thoughts?","description":"Constructing long thoughts with actions such as reflection and backtracking is a key element of journey learning. We have explored several approaches to achieve this.","sidebar":"tutorialSidebar"},"projects/o1_replicate/exploration_journey/construct_policy_tree":{"id":"projects/o1_replicate/exploration_journey/construct_policy_tree","title":"How to Construct an On-policy Reasoning Tree?","description":"Constructing a reasoning tree requires a policy model that performs single-step reasoning. Starting from a problem as the root node, the model generates possible reasoning steps as child nodes, continuing iteratively until a maximum depth is reached or the correct answer is found.","sidebar":"tutorialSidebar"},"projects/o1_replicate/exploration_journey/eval_trial":{"id":"projects/o1_replicate/exploration_journey/eval_trial","title":"How to Evaluate our Trials?","description":"In addition to testing accuracy scores using specific evaluation metrics on benchmarks, manually reviewing actual cases is a crucial step in evaluating data and models. Therefore, to provide a more intuitive way to evaluate the model\u2019s performance on specific problems, we build a visual data analysis platform using Streamlit. Specifically, our visualization platform includes the visualization of synthetic trees and their corresponding long thoughts as well as the output of the trained model. Furthermore, when visualizing results, we support detailed conditional filtering, such as filtering for correctly or incorrectly answered questions, or whether the output contains keywords indicating reflection or hesitation (e.g., \u201cwait\u201d). Additionally, we support comparison between different iterations of synthetic data and model outputs, which makes it highly intuitive and helps us easily validate whether the new round of data or models is effective.","sidebar":"tutorialSidebar"},"projects/o1_replicate/exploration_journey/human_annotation":{"id":"projects/o1_replicate/exploration_journey/human_annotation","title":"What would be an effective annotation strategy for human-ai collaboration?","description":"We have developed a human-AI pipeline that generates high-quality, long-form reasoning data based on the MATH dataset, following our \u201cjourney learning\u201d paradigm. This pipeline expands human-annotated solutions from a few lines to thousands of tokens, using key techniques to ensure efficient annotation.","sidebar":"tutorialSidebar"},"projects/o1_replicate/exploration_journey/longthought_work":{"id":"projects/o1_replicate/exploration_journey/longthought_work","title":"How does Long Thought Work?","description":"While we are still in the hypothesis stage without sufficient empirical evidence, we believe the success of O1\u2019s long-thought approach is due to journey learning, as discussed earlier. Unlike shortcut learning, journey learning allows the model to explore the entire decision-making process, much like human problem-solving. O1 can consider multiple solution paths, learn from mistakes, and develop a deeper understanding of the problem\u2014not just finding the correct answer but understanding why and how to reach it.","sidebar":"tutorialSidebar"},"projects/o1_replicate/exploration_journey/o1_thought":{"id":"projects/o1_replicate/exploration_journey/o1_thought","title":"What does O1\u2019s Thought Look Like?","description":"Official Example","sidebar":"tutorialSidebar"},"projects/o1_replicate/exploration_journey/outline":{"id":"projects/o1_replicate/exploration_journey/outline","title":"Outline","description":"This section outlines the core of our O1 replication project, guiding readers through our research journey using key questions that reflect the complexity of the process. From our initial evaluation of O1 with the OlympicArena datasets to the construction of long thoughts, our work has involved numerous attempts, iterations, and in-depth analysis of O1\u2019s capabilities.","sidebar":"tutorialSidebar"},"projects/o1_replicate/exploration_journey/reward_model":{"id":"projects/o1_replicate/exploration_journey/reward_model","title":"How to Construct Reward Model?","description":"To build an effective reward model, the first step is determining the appropriate granularity. Rather than evaluating only final results, we focus on step-level granularity to enhance LLM capabilities in reflection and backtracking. Using fine-tuning data, we distinguish solutions by line numbers to capture more detailed cognitive processes.","sidebar":"tutorialSidebar"},"projects/o1_replicate/exploration_journey/train":{"id":"projects/o1_replicate/exploration_journey/train","title":"How to Train our Models?","description":"Our experiments utilize the pre-trained language model deepseek-math-7b-base. The training process is divided into two main phases: Supervised Fine-Tuning (SFT) and Direct Preference Learning (DPO).","sidebar":"tutorialSidebar"},"projects/o1_replicate/exploration_journey/tree2thought":{"id":"projects/o1_replicate/exploration_journey/tree2thought","title":"How to Derive a Long Thought from a Reasoning Tree?","description":"Once the reasoning tree is constructed, the next step is to derive a long thought that includes trial and error, moving beyond traditional shortcuts focused solely on the correct answer.","sidebar":"tutorialSidebar"},"projects/o1_replicate/future_plan":{"id":"projects/o1_replicate/future_plan","title":"Future Plan","description":"As our O1 Replication Journey continues to evolve, our future plans are shaped by the insights gained and challenges encountered thus far. Drawing from our research timeline and the progress we\'ve made, we\'ve identified several key areas for future exploration and development:","sidebar":"tutorialSidebar"},"projects/o1_replicate/journey_learning":{"id":"projects/o1_replicate/journey_learning","title":"Journey Learning vs Shortcut Learning","description":"Many current machine learning and large language model approaches can be described as \u201cshortcut learning.\u201d This method focuses on achieving quick results by heavily relying on large amounts of data to improve performance. However, it often struggles with generalization, meaning it performs poorly in situations outside its training data, and lacks the ability to self-correct mistakes. While it has driven advancements, shortcut learning shows limitations in handling complex, dynamic, and open-ended challenges, making it less effective for developing truly intelligent AI.","sidebar":"tutorialSidebar"},"projects/o1_replicate/overview":{"id":"projects/o1_replicate/overview","title":"Overview","description":"The first project introduces a pioneering approach to artificial intelligence research, embodied in our O1 Replication Journey. In response to the announcement of OpenAI\u2019s groundbreaking O1 model, we embark on a transparent, real-time exploration to replicate its capabilities while reimagining the process of conducting and communicating AI research.","sidebar":"tutorialSidebar"},"projects/stay_tune":{"id":"projects/stay_tune","title":"Stay tuned ...","description":"","sidebar":"tutorialSidebar"}}}}')}}]);