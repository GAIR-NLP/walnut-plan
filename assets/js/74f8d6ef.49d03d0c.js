"use strict";(self.webpackChunkwalnut_plan=self.webpackChunkwalnut_plan||[]).push([[8391],{34:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var r=t(4848),i=t(8453);const s={sidebar_position:1},a="Journey Learning vs Shortcut Learning",o={id:"projects/o1_replicate/journey_learning",title:"Journey Learning vs Shortcut Learning",description:"Many current machine learning and large language model approaches can be described as \u201cshortcut learning.\u201d This method focuses on achieving quick results by heavily relying on large amounts of data to improve performance. However, it often struggles with generalization, meaning it performs poorly in situations outside its training data, and lacks the ability to self-correct mistakes. While it has driven advancements, shortcut learning shows limitations in handling complex, dynamic, and open-ended challenges, making it less effective for developing truly intelligent AI.",source:"@site/docs/projects/o1_replicate/1_journey_learning.md",sourceDirName:"projects/o1_replicate",slug:"/projects/o1_replicate/journey_learning",permalink:"/walnut-plan/docs/projects/o1_replicate/journey_learning",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Overview",permalink:"/walnut-plan/docs/projects/o1_replicate/overview"},next:{title:"Exploration Journey",permalink:"/walnut-plan/docs/category/exploration-journey"}},l={},c=[];function d(e){const n={em:"em",h1:"h1",header:"header",img:"img",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"journey-learning-vs-shortcut-learning",children:"Journey Learning vs Shortcut Learning"})}),"\n",(0,r.jsxs)(n.p,{children:["Many current machine learning and large language model approaches can be described as ",(0,r.jsx)(n.strong,{children:"\u201cshortcut learning.\u201d"})," This method focuses on achieving quick results by heavily relying on large amounts of data to improve performance. However, it often struggles with generalization, meaning it performs poorly in situations outside its training data, and lacks the ability to self-correct mistakes. While it has driven advancements, shortcut learning shows limitations in handling complex, dynamic, and open-ended challenges, making it less effective for developing truly intelligent AI."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsx)(n.tr,{children:(0,r.jsx)(n.th,{style:{textAlign:"center"},children:(0,r.jsx)(n.img,{src:t(7949).A+"",width:"1740",height:"770"})})})}),(0,r.jsx)(n.tbody,{children:(0,r.jsx)(n.tr,{children:(0,r.jsx)(n.td,{style:{textAlign:"center"},children:(0,r.jsx)(n.em,{children:"A paradigm shift from \u201cshortcut learning\u201d to \u201cjourney learning\u201d. A searching tree for reasoning tasks. For the math problem-solving task, the root node represents the initial problem, while the leaf nodes are final conclusions. Green nodes indicate correct answers, and red nodes incorrect ones. Traditionally, learning focused on supervised training of a direct root-to-leaf shortcut path. This work, however, explores supervised learning of the entire exploration path, encompassing trial-and-error and correction processes."})})})})]}),"\n",(0,r.jsxs)(n.p,{children:["To address these limitations, we propose a new approach: ",(0,r.jsx)(n.strong,{children:"\u201cjourney learning.\u201d"})," This paradigm goes beyond the traditional focus on specific tasks and emphasizes continuous progress through learning, reflection, and adaptation. AI systems that follow this method can evolve over time, improving their ability to handle real-world complexities. Unlike shortcut learning, journey learning equips AI with the capacity to adapt, backtrack, and refine its understanding, aiming to create more human-like intelligence."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:(0,r.jsx)(n.strong,{children:"Characteristic"})}),(0,r.jsx)(n.th,{children:(0,r.jsx)(n.strong,{children:"Shortcut Learning"})}),(0,r.jsx)(n.th,{children:(0,r.jsx)(n.strong,{children:"Journey Learning"})})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Learning Depth"})}),(0,r.jsx)(n.td,{children:"Surface features and simple correlations"}),(0,r.jsx)(n.td,{children:"Deep causal relationships and underlying principles"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Reasoning Ability"})}),(0,r.jsx)(n.td,{children:"Limited, struggles with complex reasoning"}),(0,r.jsx)(n.td,{children:"Powerful, demonstrates human-like reasoning"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Self-Improvement"})}),(0,r.jsx)(n.td,{children:"Lacks self-correction mechanisms"}),(0,r.jsx)(n.td,{children:"Continuous self-assessment and improvement"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Generalization"})}),(0,r.jsx)(n.td,{children:"Limited, easily affected by data distribution changes"}),(0,r.jsx)(n.td,{children:"Strong, can handle new situations"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Innovation Capacity"})}),(0,r.jsx)(n.td,{children:"Limited, struggles to solve new problems"}),(0,r.jsx)(n.td,{children:"High, can generate innovative solutions"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Data Dependency"})}),(0,r.jsx)(n.td,{children:"Highly dependent on large training datasets"}),(0,r.jsx)(n.td,{children:"More focused on quality and learning strategies"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Interpretability"})}),(0,r.jsx)(n.td,{children:'Poor, often seen as a "black box"'}),(0,r.jsx)(n.td,{children:"Better, can track internal reasoning processes"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Ethical Considerations"})}),(0,r.jsx)(n.td,{children:"May unintentionally amplify data biases"}),(0,r.jsx)(n.td,{children:"Easier to implement ethical constraints and adjustments"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Security"})}),(0,r.jsx)(n.td,{children:"Vulnerable to adversarial attacks"}),(0,r.jsx)(n.td,{children:"More robust, able to identify potential threats"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Long-term Value"})}),(0,r.jsx)(n.td,{children:"Quick results in specific tasks"}),(0,r.jsx)(n.td,{children:"Paves the way for AGI development"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Human Analogy"})}),(0,r.jsx)(n.td,{children:"Exam-oriented education, crash courses"}),(0,r.jsx)(n.td,{children:"Comprehensive education, lifelong learning"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Comparison between Shortcut Learning and Journey Learning."})}),"\n",(0,r.jsx)(n.p,{children:"This shift to journey learning opens up new possibilities in AI research, enabling the creation of systems that can not only perform tasks but also reason and grow, making them more capable of engaging with humans across various domains."})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},7949:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/journey_vs_shortcut-ae058691ab7c533988922ca59ea1b6bf.png"},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var r=t(6540);const i={},s=r.createContext(i);function a(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);