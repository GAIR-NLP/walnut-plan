"use strict";(self.webpackChunkwalnut_plan=self.webpackChunkwalnut_plan||[]).push([[45],{6361:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>d,frontMatter:()=>s,metadata:()=>a,toc:()=>l});var o=n(4848),r=n(8453);const s={sidebar_position:1},i="How to Construct an On-policy Reasoning Tree?",a={id:"projects/o1_replicate/exploration_journey/construct_policy_tree",title:"How to Construct an On-policy Reasoning Tree?",description:"Constructing a reasoning tree requires a policy model that performs single-step reasoning. Starting from a problem as the root node, the model generates possible reasoning steps as child nodes, continuing iteratively until a maximum depth is reached or the correct answer is found.",source:"@site/docs/projects/o1_replicate/2_exploration_journey/06_construct_policy_tree.md",sourceDirName:"projects/o1_replicate/2_exploration_journey",slug:"/projects/o1_replicate/exploration_journey/construct_policy_tree",permalink:"/walnut-plan/docs/projects/o1_replicate/exploration_journey/construct_policy_tree",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"How to Construct Reward Model?",permalink:"/walnut-plan/docs/projects/o1_replicate/exploration_journey/reward_model"},next:{title:"How to Derive a Long Thought from a Reasoning Tree?",permalink:"/walnut-plan/docs/projects/o1_replicate/exploration_journey/tree2thought"}},c={},l=[];function p(e){const t={h1:"h1",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"how-to-construct-an-on-policy-reasoning-tree",children:"How to Construct an On-policy Reasoning Tree?"})}),"\n",(0,o.jsx)(t.p,{children:"Constructing a reasoning tree requires a policy model that performs single-step reasoning. Starting from a problem as the root node, the model generates possible reasoning steps as child nodes, continuing iteratively until a maximum depth is reached or the correct answer is found."}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.strong,{children:"Policy Model and Step Segmentation:"})," To structure reasoning steps, we used the dataset from Abel, fine-tuning DeepSeekMath-7B-Base to create Abel-DSMath. This model generates reasoning steps clearly segmented by lines, which allows for controlled and precise stepwise reasoning."]}),"\n"]}),"\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.strong,{children:"Reward Model and Pruning:"})," Generating a reasoning tree is computationally expensive. To address this, we implemented beam search to prune erroneous steps and improve efficiency. Two reward models were tested: math-shepherd and o1-mini. While math-shepherd scores each step\u2019s probability of correctness, o1-mini offers more robust step-level rewards, directly indicating whether a reasoning step is correct. By selecting only the K highest-scoring steps in each iteration, we drastically reduce the number of generated steps, making the process more efficient."]}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>a});var o=n(6540);const r={},s=o.createContext(r);function i(e){const t=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),o.createElement(s.Provider,{value:t},e.children)}}}]);