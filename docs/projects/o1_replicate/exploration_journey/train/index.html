<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-projects/o1_replicate/exploration_journey/train" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">How to Train our Models? | Walnut Plan</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://gair-nlp.github.io/walnut-plan/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://gair-nlp.github.io/walnut-plan/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://gair-nlp.github.io/walnut-plan/docs/projects/o1_replicate/exploration_journey/train"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="How to Train our Models? | Walnut Plan"><meta data-rh="true" name="description" content="Our experiments utilize the pre-trained language model deepseek-math-7b-base. The training process is divided into two main phases: Supervised Fine-Tuning (SFT) and Direct Preference Learning (DPO)."><meta data-rh="true" property="og:description" content="Our experiments utilize the pre-trained language model deepseek-math-7b-base. The training process is divided into two main phases: Supervised Fine-Tuning (SFT) and Direct Preference Learning (DPO)."><link data-rh="true" rel="icon" href="/walnut-plan/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://gair-nlp.github.io/walnut-plan/docs/projects/o1_replicate/exploration_journey/train"><link data-rh="true" rel="alternate" href="https://gair-nlp.github.io/walnut-plan/docs/projects/o1_replicate/exploration_journey/train" hreflang="en"><link data-rh="true" rel="alternate" href="https://gair-nlp.github.io/walnut-plan/docs/projects/o1_replicate/exploration_journey/train" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/walnut-plan/blog/rss.xml" title="Walnut Plan RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/walnut-plan/blog/atom.xml" title="Walnut Plan Atom Feed"><link rel="stylesheet" href="/walnut-plan/assets/css/styles.10bf9707.css">
<script src="/walnut-plan/assets/js/runtime~main.03f93cc2.js" defer="defer"></script>
<script src="/walnut-plan/assets/js/main.cf76a5c6.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/walnut-plan/"><div class="navbar__logo"><img src="/walnut-plan/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/walnut-plan/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Walnut Plan</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/walnut-plan/docs/intro">Content</a><a class="navbar__item navbar__link" href="/walnut-plan/key_recent_works">Key Recent Works</a><a class="navbar__item navbar__link" href="/walnut-plan/team">Team</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/gair-nlp" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/walnut-plan/docs/intro">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/walnut-plan/docs/category/projects">Projects</a><button aria-label="Collapse sidebar category &#x27;Projects&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/walnut-plan/docs/category/o1-replication-journey">O1 Replication Journey</a><button aria-label="Collapse sidebar category &#x27;O1 Replication Journey&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/overview">Overview</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/journey_learning">Journey Learning vs Shortcut Learning</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/walnut-plan/docs/category/exploration-journey">Exploration Journey</a><button aria-label="Collapse sidebar category &#x27;Exploration Journey&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/exploration_journey/outline">Outline</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/exploration_journey/o1_thought">What does O1’s Thought Look Like?</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/exploration_journey/longthought_work">How does Long Thought Work?</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/exploration_journey/construct_longthought">How to Construct Long Thoughts?</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/exploration_journey/reward_model">How to Construct Reward Model?</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/exploration_journey/construct_policy_tree">How to Construct an On-policy Reasoning Tree?</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/exploration_journey/tree2thought">How to Derive a Long Thought from a Reasoning Tree?</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/exploration_journey/eval_trial">How to Evaluate our Trials?</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/exploration_journey/train">How to Train our Models?</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/exploration_journey/human_annotation">What would be an effective annotation strategy for human-ai collaboration?</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/examples">Examples</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/o1_replicate/future_plan">Future Plan</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/walnut-plan/docs/projects/stay_tune">Stay tuned ...</a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/walnut-plan/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/walnut-plan/docs/category/projects"><span itemprop="name">Projects</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/walnut-plan/docs/category/o1-replication-journey"><span itemprop="name">O1 Replication Journey</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/walnut-plan/docs/category/exploration-journey"><span itemprop="name">Exploration Journey</span></a><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">How to Train our Models?</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>How to Train our Models?</h1></header>
<p>Our experiments utilize the pre-trained language model deepseek-math-7b-base. The training process is divided into two main phases: Supervised Fine-Tuning (SFT) and Direct Preference Learning (DPO).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="phase-1-supervised-fine-tuning-sft">Phase 1: Supervised Fine-Tuning (SFT)<a href="#phase-1-supervised-fine-tuning-sft" class="hash-link" aria-label="Direct link to Phase 1: Supervised Fine-Tuning (SFT)" title="Direct link to Phase 1: Supervised Fine-Tuning (SFT)">​</a></h3>
<p>The SFT process consists of two stages:</p>
<ol>
<li>
<p><strong>ShortCut Learning:</strong> In this initial stage, we focus on fine-tuning the model using responses that include only the correct intermediate steps and the final correct answer. We fine-tune Deepseek-math-7b-base on the Abel dataset, which comprises 120k examples, and the PRM800K dataset. For each question in PRM800K, we utilize a single correct step-by-step solution, discarding responses that do not lead to the final answer. This results in a total of 6,998 examples for fine-tuning. During this stage, we conduct fine-tuning for one epoch on each dataset, primarily aiming to familiarize the model with the desired response format.</p>
</li>
<li>
<p><strong>Journey Learning:</strong> In this second stage, we further fine-tune the initial stage SFT model using the long thoughts we constructed, which comprise 327 examples. This phase is designed to enhance the model&#x27;s ability to detect errors, incorporate reflections, execute corrections, and perform backtracking. By training on long thoughts that include not only the correct reasoning paths but also erroneous trials, we aim to equip the model with a deeper understanding of the complexities involved in longer reasoning chains. As a comparison, we also fine-tune the model on the corresponding shortCut generated from the same reasoning tree, which also consists of 327 examples. Both the long thought SFT and shortCut SFT settings are trained for 3 epochs on these 327 examples.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="phase-2-direct-preference-learning-dpo">Phase 2: Direct Preference Learning (DPO)<a href="#phase-2-direct-preference-learning-dpo" class="hash-link" aria-label="Direct link to Phase 2: Direct Preference Learning (DPO)" title="Direct link to Phase 2: Direct Preference Learning (DPO)">​</a></h3>
<p>In this phase, we generate 20 responses per question from the MATH Train dataset, a re-divided dataset from PRM800k that includes 12,000 examples, using nucleus sampling with <code>top_p = 0.95</code> and temperature <code>T = 0.7</code>. These 20 responses are categorized into positive and negative responses based on the correctness of the final answer. From these, we randomly select 5 positive responses and 5 negative responses to create 5 preference pairs. We then train the model using these preference pairs with DPO loss, allowing it to learn from the comparison of correct and incorrect answers.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results<a href="#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h3>
<table><thead><tr><th></th><th>deepseek-sft-abel</th><th>deepseek-sft-prm800k</th></tr></thead><tbody><tr><td><strong>SFT-phase1</strong></td><td>0.372</td><td>0.290</td></tr><tr><td><strong>SFT-phase2-shortcutLearning</strong></td><td>0.386</td><td>0.348</td></tr><tr><td><strong>SFT-phase2-journeyLearning</strong></td><td>0.470</td><td>0.428</td></tr><tr><td><strong>DPO</strong></td><td>0.472</td><td>0.440</td></tr></tbody></table>
<p><em>Table: Training Results on MATH Test Set</em></p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/walnut-plan/docs/projects/o1_replicate/exploration_journey/eval_trial"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">How to Evaluate our Trials?</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/walnut-plan/docs/projects/o1_replicate/exploration_journey/human_annotation"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">What would be an effective annotation strategy for human-ai collaboration?</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#phase-1-supervised-fine-tuning-sft" class="table-of-contents__link toc-highlight">Phase 1: Supervised Fine-Tuning (SFT)</a></li><li><a href="#phase-2-direct-preference-learning-dpo" class="table-of-contents__link toc-highlight">Phase 2: Direct Preference Learning (DPO)</a></li><li><a href="#results" class="table-of-contents__link toc-highlight">Results</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 GAIR@SJTU.</div></div></div></footer></div>
</body>
</html>